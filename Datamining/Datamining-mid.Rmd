---
title: "Datamining-mid"
output: 
  html_document : 
    toc : TRUE
    theme: united
    toc_depth : 3
    toc_float: 
      collapsed : false
---

# *1*

### 데이터 세팅
```{r}
library(FNN)
library(glmnet)
library(quantreg)
load("C:/Users/so/Desktop/데이터마이닝/Cheongpa2.Rdata")
head(Cheongpa2) # 이 데이터는 2018.12.08~2020.09.30  #658개의 자료 
set.seed(1) 
c_data <- Cheongpa2
```


### 데이터 분할
```{r}
nn = floor(nrow(c_data)*0.6)
nn2 = floor(nrow(c_data)*0.2)
df.train = c_data[1:nn,]  #394개
df.val = c_data[(nn+1):(nn+nn2),] #131개
df.test = c_data[(nn+nn2+1):nrow(c_data),]  #133개

x.train = df.train[,-c(1,2)]
x.val = df.val[,-c(1,2)]
x.test = df.test[,-c(1,2)]

y.train = df.train[,2]
y.val = df.val[,2]
y.test = df.test[,2]
```
데이터를 훈련(training) : 검증(validation) : 테스트(test) 세트로 분할하였다. 6:2:2로 분할하였으며, 전체 658개의 자료에 대하여 394개는 training data, 131개는 validation data, 133개는 test data에 해당한다. 현재 데이터는 시계열 데이터이므로 순서대로 분할하는 것에 주의하였다.

```{r}
head(df.train,3)
tail(df.train,3)
```
train data에 속하는 Date는 2018-12-08 ~ 2020-01-09이다.

```{r}
head(df.val,3)
tail(df.val,3)
```
validation data에 속하는 Date는 2020-01-10 ~ 2020-05-19이다.

```{r}
head(df.test,3)
tail(df.test,3)
```
train data에 속하는 Date는 2020-05-20 ~ 2020-09-30이다.

***
### 고려한 오류측도
고려한 오류측도 3가지는 다음과 같다. (출처 : 위키피디아)

* RMSE
![RMSE](./data/RMSE.png)

* MAE
![MAE](./data/MAE.png)

* MAPE
![MAPE](./data/MAPE.png)

***

### 나이브 벤치마크
```{r}
## 나이브 벤치마크
yhat.naive = mean(y.train)
rmse.naive = sqrt(mean((y.val-yhat.naive)^2))
mae.naive = mean(abs(y.val-yhat.naive))
mape.naive = mean(abs((y.val-yhat.naive)/y.val))
error.naive = c(rmse.naive, mae.naive, mape.naive)
names(error.naive) = c("rmse", "mae", "mape")
error.naive
```

첫 번째 모형으로 나이브 벤치마크 모형을 고려하였다. 출력된 것은 나이브 벤치마크 모형을 적합했을 때의 검증세트 오류를 의미한다.

### k-최근접이웃
```{r}
K= 30
rmse.knn = numeric(K)
mae.knn = numeric(K)
mape.knn = numeric(K)
for (kk in 1:K) {
  yhat.knn = knn.reg(train=x.train, y = y.train, test=x.val, k = kk)
  rmse.knn[kk] = sqrt(mean((y.val-yhat.knn$pred)^2))  #rmse
  mae.knn[kk] = mean(abs(y.val-yhat.knn$pred)) # mae
  mape.knn[kk] = mean(abs((y.val-yhat.knn$pred)/y.val)) #mape
}
optimal.k.knn.rmse = which.min(rmse.knn)
optimal.k.knn.mae = which.min(mae.knn)
optimal.k.knn.mape = which.min(mape.knn)

optimal.k = c(optimal.k.knn.rmse, optimal.k.knn.mae, optimal.k.knn.mape)
names(optimal.k) <- c("k(rmse)", "k(mae)", "k(mape)")
optimal.k
```

3개의 오류 측도를 고려했을 때, 가장 작은 값이 나오는 k값을 출력해보았다. mae와 mape는 같은 k값에서 가장 작은 값을 가지는 것을 볼 수 있다.

```{r}
error.knn = c(rmse.knn[optimal.k.knn.rmse], mae.knn[optimal.k.knn.mae], mape.knn[optimal.k.knn.mape])
names(error.knn) <- c("rmse", "mae", "mape")
error.knn
```

k-최근접 이웃 모형을 적합했을 때, 각각의 오류 측도에 해당하는 검증세트 오류이다. 

```{r}
par(mfrow = c(1,3))
plot(1:30, rmse.knn)
plot(1:30, mae.knn)
plot(1:30, mape.knn)
```

각각의 k값(1~30)에 대한 오류측도를 그려보았다.

### 선형회귀
```{r}
fit.lm = lm(y.train ~ as.matrix(x.train))
yhat.lm = as.matrix(cbind(1, x.val)) %*% coef(fit.lm)

rmse.lm = sqrt(mean((y.val-yhat.lm)^2))
mae.lm = mean(abs(y.val-yhat.lm))
mape.lm = mean(abs((y.val-yhat.lm)/y.val))

error.lm = c(rmse.lm, mae.lm, mape.lm)
error.lm
```

선형 회귀 모형을 적합했을 때, 각각의 오류 측도에 해당하는 검증세트 오류이다.

### 능형 선형회귀
```{r}
grid = 2^seq(from = 50, to = -49)
rmse.ridge = numeric(length(grid))
mae.ridge = numeric(length(grid))
mape.ridge = numeric(length(grid))

fit.ridge = glmnet(x = as.matrix(x.train), y = y.train, family = "gaussian", alpha = 0, lambda = grid)

for (g in 1:length(grid)) {
  yhat.ridge = as.matrix(cbind(1, x.val)) %*% coef(fit.ridge)[,g]
  rmse.ridge[g] = sqrt(mean((y.val-yhat.ridge)^2))
  mae.ridge[g] = mean(abs(y.val-yhat.ridge))
  mape.ridge[g] = mean(abs((y.val-yhat.ridge)/y.val))
}

optimal.lambda.ridge.rmse = grid[which.min(rmse.ridge)]
optimal.lambda.ridge.mae = grid[which.min(mae.ridge)]
optimal.lambda.ridge.mape = grid[which.min(mape.ridge)]

optimal.lambda.ridge = c(optimal.lambda.ridge.rmse, optimal.lambda.ridge.mae, optimal.lambda.ridge.mape)
optimal.lambda.ridge

error.ridge = c(rmse.ridge[which.min(rmse.ridge)], mae.ridge[which.min(mae.ridge)], mape.ridge[which.min(mape.ridge)])
error.ridge  
```

능형 선형회귀 모형을 적합했을 때, 각각의 오류 측도에 해당하는 최적의 람다값과 검증세트 오류이다.


```{r}
par(mfrow = c(1,3))
plot(log2(grid), rmse.ridge)
plot(log2(grid), mae.ridge)
plot(log2(grid), mape.ridge)
```

각각의 grid값(2^50 ~ 2^(-49))에 대한 오류측도를 그려보았다.

### 라쏘 선형회귀
```{r}
rmse.lasso = numeric(length(grid))
mae.lasso = numeric(length(grid))
mape.lasso = numeric(length(grid))

fit.lasso = glmnet(x = as.matrix(x.train), y = y.train, family = "gaussian", alpha = 1, lambda = grid)

for (g in 1:length(grid)) {
  yhat.lasso = as.matrix(cbind(1, x.val)) %*% coef(fit.lasso)[,g]
  rmse.lasso[g] = sqrt(mean((y.val-yhat.lasso)^2))
  mae.lasso[g] = mean(abs(y.val-yhat.lasso))
  mape.lasso[g] = mean(abs((y.val-yhat.lasso)/y.val))
}

optimal.lambda.lasso.rmse = grid[which.min(rmse.lasso)]
optimal.lambda.lasso.mae = grid[which.min(mae.lasso)]
optimal.lambda.lasso.mape = grid[which.min(mape.lasso)]

optimal.lambda.lasso = c(optimal.lambda.lasso.rmse, optimal.lambda.lasso.mae, optimal.lambda.lasso.mape)
optimal.lambda.lasso

error.lasso = c(rmse.lasso[which.min(rmse.lasso)], mae.lasso[which.min(mae.lasso)], mape.lasso[which.min(mape.lasso)])
error.lasso
```

라쏘 선형회귀 모형을 적합했을 때, 각각의 오류 측도에 해당하는 최적의 람다값과 검증세트 오류이다

```{r}
par(mfrow = c(1,3))
plot(log2(grid), rmse.lasso)
plot(log2(grid), mae.lasso)
plot(log2(grid), mape.lasso)
```

각각의 grid값(2^50 ~ 2^(-49))에 대한 오류측도를 그려보았다.

### error
```{r}
errors <- rbind(error.naive, error.knn, error.lm, error.ridge, error.lasso)
colnames(errors) <- c("rmse", "mae", "mape")
errors #ridge가 제일 좋음
```

모든 모형의 rmse, mae, mape의 값이다. naive모형은 rmse, mae, mape 각 오류 측도에서 모두 제일 큰 값을 가지는 것을 볼 수 있다. rmse 측도는 능형 선형 모형에서 가장 작은 값을 가지는 것을 알 수 있다. mae에서는 선형회귀 모형, 능형 선형회귀 모형, 라쏘 선형회귀 모형이 비슷한 값을 가진다. 제일 작은 값을 가지는 모형은 능형 선형회귀 모형이다. mape 측도에서도 역시 선형회귀 모형, 능형 선형회귀 모형, 라쏘 선형모형이 비슷한 값을 가지는 것을 알 수있다. 제일 작은 값을 가지는 모형은 능형 선형회귀 모형이다. 따라서 3개의 오류측도를 고려했을 때, rmse에서는 k-최근접이웃 모형이, mae와 mape에서는 능형 선형회귀 모형이 오류 측도를 최소화하는 것을 볼 수 있다. 

따라서 능형 선형회귀 모형을 예측을 위한 모형으로 결정한다. 따라서 능형 선형회귀의 람다를 결정해야 한다.

### 시각화
먼저 앞에서 구한 다섯 개의 예측값들을 이용하여 시계열 그래프를 그려본다.

```{r}
par(mfrow = c(2,3)) # 합쳐서 그렸을 때, orange, pink, blue는 거의 겹져진다.
ticks = as.Date(c("2020-02-01", "2020-03-01","2020-04-01","2020-05-01"))
ticklabels = as.character(ticks)


plot(x=df.val$Date, y=df.val$Today, type = "l", xlab="Date", ylab="Today", main = "naive")
lines(x=df.val$Date, y=rep(yhat.naive,nrow(df.val)), col = "green")
axis(3, at = ticks, labels = ticklabels)
abline(v=ticks, lty = 2, col = "grey")

plot(x=df.val$Date, y=df.val$Today, type = "l", xlab="Date", ylab="Today", main = "knn")
lines(x=df.val$Date, y=yhat.knn$pred, col = "red")
axis(3, at = ticks, labels = ticklabels)
abline(v=ticks, lty = 2, col = "grey")

plot(x=df.val$Date, y=df.val$Today, type = "l", xlab="Date", ylab="Today", main = "lm")
lines(x=df.val$Date, y=yhat.lm, col = "blue")
axis(3, at = ticks, labels = ticklabels)
abline(v=ticks, lty = 2, col = "grey")

plot(x=df.val$Date, y=df.val$Today, type = "l", xlab="Date", ylab="Today", main = "ridge")
lines(x=df.val$Date, y=yhat.ridge, col = "orange")
axis(3, at = ticks, labels = ticklabels)
abline(v=ticks, lty = 2, col = "grey")

plot(x=df.val$Date, y=df.val$Today, type = "l", xlab="Date", ylab="Today", main = "lasso")
lines(x=df.val$Date, y=yhat.lasso, col = "pink")
axis(3, at = ticks, labels = ticklabels)
abline(v=ticks, lty = 2, col = "grey")

# 한 화면에 그리기
plot(x=df.val$Date, y=df.val$Today, type = "l", xlab="Date", ylab="Today", main = "all")
lines(x=df.val$Date, y=rep(yhat.naive,nrow(df.val)), col = "green")
axis(3, at = ticks, labels = ticklabels)
abline(v=ticks, lty = 2, col = "grey")
lines(x=df.val$Date, y=yhat.knn$pred, col = "red")
lines(x=df.val$Date, y=yhat.lm, col = "blue")
lines(x=df.val$Date, y=yhat.ridge, col = "orange")
lines(x=df.val$Date, y=yhat.lasso, col = "pink")
```
원래의 값은 검은색 선을 이용하여 표시하였고, 예측된 값들을 서로 다른 색깔을 이용하여 그림을 그려보았다. 마지막 6번째 그림은 첫 번째 그림에서 다섯 번째 그림을 모두 합친 그림이다. 나이브 벤치마크를 이용했을 때는 시계열의 예측과 매우 멀어지는 것을 볼 수 있다. 이 모형을 제외하고는 거의 비슷한 그림이 그려진다고 볼 수 있다. 

### lambda
앞서서 고려한 다섯 개의 모형족 중 능형 선형회귀 모형을 적합하기로 결정하였다. 예측을 위하여 최적의 lambda 값을 결정해보자.

```{r}
optimal.lambda.ridge
```

앞에서 고려하였던 lambda 값들 중에서 rmse, mae, mape의 오류측도를 최소화 하는 람다값이다. 여기서 mae와 mape를 최소화하는 람다값이 동일함을 알 수 있다. 따라서 이 값을 최종 모형의 람다값으로 결정한다. 

```{r}
lambda.final = optimal.lambda.ridge[2]
```


### 최종 - 테스트세트 오류
```{r}
data_vt <- rbind(df.train, df.val)
x.vt <- data_vt[,-c(1,2)]
y.vt <- data_vt[,2]

fit.ridge.final = glmnet(x = as.matrix(x.vt), y = y.vt, family = "gaussian", alpha = 0, lambda = lambda.final)
fit.ridge.final

yhat.final = as.matrix(cbind(1, x.test)) %*% coef(fit.ridge.final)
mse.final = sqrt(mean((y.test-yhat.final)^2))
mae.final = mean(abs(y.test-yhat.final))
mape.final = mean(abs((y.test-yhat.final)/y.test))

error.final = c(mse.final, mae.final, mape.final)
names(error.final) <-c("rmse", "mae", "mape")
error.final
```

위에서 고른 최종 모형으로 ridge 모형과 이에 대한 람다 값으로 lambda.final을 정하였다. 이 모형에서의 테스트 세트 오류를 제시하기 위하여 training data와 validation data를 합쳐서 다시 적합하였다. test data를 이용하였을 때 나오는 rmse값은 약 1336.6, mae값은 약 1020.9, mape값은 0.029의 값이 나온다. 
```{r}
errors
```

앞에서 구한 에러들을 다시 보면, 나이브 벤치마크 모형은 5010정도이고, k-최근접이웃은 약 1500, 나머지는 약 1200점대 정도가 나왔다. 이와 비교했을 때, 검증세트에서의 능형 선형회귀모형 오류보다는 조금 크게 측정되는 것을 볼 수 있다. mae에서는 나이브 벤치마크 모형을 제외하고 볼 때, 다른 모형들보다는 비교적 크게 측정된다. mape에서는 모든 모형들 보다도 가장 작게 추정되는 것으로 보인다. 

따라서 최종모형은 람다 값으로 0.25를 사용한 능형 선형회귀 모형이다.

```{r}
par(mfrow = c(1,1))
ticks = as.Date(c("2020-06-01", "2020-07-01","2020-08-01","2020-09-01"))
ticklabels = as.character(ticks)

plot(x=df.test$Date, y=df.test$Today, type = "l", xlab="Date", ylab="Today")
lines(x=df.test$Date, y=yhat.final, col = "blue")
axis(3, at = ticks, labels = ticklabels)
abline(v=ticks, lty = 2, col = "grey")
```

최종 모형을 통한 예측값을 원래의 값과 겹쳐서 그려보았을 때, 거의 원래의 값들과 비슷한 값을 예측하는 것으로 볼 수 있다.

# *2*

## (b)

### 함수 작성
```{r}
set.seed(1)
glm.poisson <- function(Xmat, y) {

MAXITER = 1000
tol = 10^-8

theta.old = c(0.7,-1,0.3)
theta.old = as.matrix(theta.old)

for (t in 1:MAXITER) {
  cat(sprintf("============ Iteration %d ==========\n", t))
  
  dpi <- ((t(Xmat) %*% exp(Xmat %*% theta.old)) - (t(Xmat) %*% y)) / n
  d2pi <- matrix(0, ncol=3, nrow=3)

  for (i in 1:n) {
    tmp <- as.matrix(Xmat[i,]) %*% exp(as.matrix(t(theta.old)) %*% as.matrix(Xmat[i,])) %*% as.matrix(t(Xmat[i,]))
    d2pi <- d2pi + tmp # 3x3
  }
  
  d2pi <- d2pi/n
  d2pi2 <- solve(d2pi) # 역행렬 계산
  
  theta.new <- theta.old - c( d2pi2 %*% (dpi) )
  
  cat(sprintf("theta.old= (%.5f, %.5f, %.5f)\n", theta.old[1], theta.old[2], theta.old[3]))
  cat(sprintf("theta.new= (%.5f, %.5f, %.5f)\n\n", theta.new[1], theta.new[2], theta.new[3]))
  
  # diff는 theta.new와 theta.old간 유클리드 거리로 정의하였음
  diff = sqrt(sum((theta.new - theta.old)^2))
  
  cat(sprintf("L2 difference between theta.new and theta.old: %.8f\n\n", diff)) 
  
  if (diff < tol) { 
    cat(sprintf("Fisher scoring algorithm converged with %d iterations\n", t))
    theta.hat = theta.new
    break
  }
  theta.old <- theta.new
  
  # 수렴 실패하였을 경우 메시지 출력
  if (t == MAXITER) cat("Did not converge\n")
}
rownames(theta.hat) <- c("beta0", "beta1", "beta2")
return (theta.hat)
}
```

### 호출1. n = 10
```{r}
n = 10
X1 = rnorm(n)
X2 = rnorm(n)
# the true coefficients. beta0 = 1, beta1 = -2, beta2 = 1
theta.true = c(1, -2, 1)
# beta0 + beta1 * X1 + beta2 * X2
x_theta = theta.true[1] + theta.true[2] * X1 + theta.true[3] * X2
y = rpois(n=n, lambda = exp(x_theta))
Xmat = cbind(1, X1, X2)

n10 <- glm.poisson(Xmat, y) # 호출
```

### 호출2. n = 100
```{r}
n = 100
X1 = rnorm(n)
X2 = rnorm(n)
# the true coefficients. beta0 = 1, beta1 = -2, beta2 = 1
theta.true = c(1, -2, 1)
# beta0 + beta1 * X1 + beta2 * X2
x_theta = theta.true[1] + theta.true[2] * X1 + theta.true[3] * X2
y = rpois(n=n, lambda = exp(x_theta))
Xmat = cbind(1, X1, X2)

n100 <- glm.poisson(Xmat, y) # 호출
```

### 호출2. n = 1000
```{r}
n = 1000
X1 = rnorm(n)
X2 = rnorm(n)
# the true coefficients. beta0 = 1, beta1 = -2, beta2 = 1
theta.true = c(1, -2, 1)
# beta0 + beta1 * X1 + beta2 * X2
x_theta = theta.true[1] + theta.true[2] * X1 + theta.true[3] * X2
y = rpois(n=n, lambda = exp(x_theta))
Xmat = cbind(1, X1, X2)

n1000 <- glm.poisson(Xmat, y) # 호출
```

```{r}
n10
n100
n1000
```

표본 크기를 10, 100, 1000으로 키워보면, 표본 크기가 커짐에 따라 참 계수인 1, -2, 1에 가까워지는 것을 확인할 수 있다.

### 테스트
```{r}
# glm 체크
fit <- glm(y~ X1 + X2, family = poisson)
summary(fit)
```

glm을 이용하여 theta hat을 추정해보면, 계수가 1.010555, -1.999874, 0.994894로 위에서 표본 크기가 1000일 때의 계수와 거의 같은 것을 볼 수 있다. 따라서 알고리즘이 잘 작동한다고 볼 수있다. 

## (c)
```{r}
p <- glm(y.train~as.matrix(x.train), family = poisson(link=log))
yhat.p = exp(as.matrix(cbind(1, x.val)) %*% coef(p))
p.rmse <- sqrt(mean((y.val-yhat.p)^2))
p.mae <- mean(abs(y.val-yhat.p))
p.mape <- mean(abs((y.val-yhat.p)/y.val))
error.p <- c(p.rmse, p.mae, p.mape)
names(error.p) <- c("rmse", "mae", "mape")
error.p
```

로그선형모형을 통해 예측한 값을 이용하여 rmse, mae, mape를 출력하였다. rmse는 1242.975, mae는 924.6179, mape는 0.02979216이다.
아래는 앞서 1번에서 구한 검증세트 오류들이다.

```{r}
errors
```

이 값들과 비교하여 볼 때, 로그선형모형을 이용한 오류들이 mae, mape에서 더 작게 추정되는 것을 볼 수 있다. 3개의 오류측도 중 두 개에서 가장 작은 검증 오류를 보이므로 y를 연속형 확률변수로 가정하지 않고 count type으로 간주하는 것이 더 적절하다고 볼 수 있다.

```{r}
plot(x=df.val$Date, y=df.val$Today, type = "l", xlab="Date", ylab="Today")
lines(x=df.val$Date, y=yhat.p, col = "blue", type= "l")
```

로그선형모형을 이용한 예측값들을 이용하여 시계열 그림에 겹쳐서 그려보았다. 가장 낮아지는 부분에서 살짝 예측이 떨어지기는 하지만, 전반적인 주기성은 잘 보여진다고 볼 수 있다.

# *3*

## (b)
```{r}
make <- numeric()
for (i in 1:100) {
  set.seed(i)
  x <- rnorm(100)
  y = 2 + 3 * x
  t = quantile(x, 0.1)
  ft = 2 + 3 * t
  make <- rbind(make, c(ft, t))
}
colnames(make) <- c("y","x")
make <- data.frame(make)
head(make)
```

데이터셋을 생성하기 위하여 100개의 x값들을 정규분포에서 뽑은 후, 이에 대한 y값들 중에서 0.1분위수에 해당하는 x값과 y값을 make 벡터에 저장하였다. 즉 2 + 3x = y를 가지는 100개의 sample 중 0.1분위수에 해당하는 점들을 찾아 make에 추가하는 것을 100회 반복한다.

```{r}
rq(y~x, data = make, tau=0.1)
```

rq함수 사용 결과 이 데이터셋에서 2와 3을 잘 추정하는 것을 알 수 있다. 즉, 분위수 0.1에 해당하는 값이 나오도록 회귀 계수가 추정되는 것을 알 수 있다. 

## (c)
```{r}
taus <- c(0.1, 0.3, 0.5, 0.7, 0.9)
tau.estimate <- rq(y.train~as.matrix(x.train), tau = taus)
yhat <- numeric()
yhats <- numeric()
for (i in 1:length(taus)) {
  yhat <- (as.matrix(cbind(1, x.val))) %*% as.matrix(tau.estimate$coefficients[,i])
  yhats <- cbind(yhats, yhat)
}
plot(x=df.val$Date, y=df.val$Today, type = "l", xlab="Date", ylab="Today")
lines(x=df.val$Date, y=yhats[,1], col = "pink")
lines(x=df.val$Date, y=yhats[,2], col = "red")
lines(x=df.val$Date, y=yhats[,3], col = "blue")
lines(x=df.val$Date, y=yhats[,4], col = "green")
lines(x=df.val$Date, y=yhats[,5], col = "purple")
```

tau를 0.1, 0.3, 0.5, 0.7, 0.9로 고려하였을 때 나오는 예측값들을 yhats에 저장하였다. 이후, 이 5개의 예측값들을 색상을 다르게 하여 그려보았다. 참값은 검은색으로 표현하였다. 이것을 볼 때, tau값이 0.1인 값이 가장 낮아서 0.1분위수를 잘 추정하는 것으로 보이며, tau가 0.9인 값은 거의 높은 값을 가지므로 0.9분위수를 잘 예측한다는 것을 알 수 있다. 따라서 굉장히 예측의 범위가 커보이는 그림이 나타나게 되었다.

## (e)
```{r}
yhats.boot <- numeric(1000)
for (i in 1:1000) {
   k <- sample(1:nrow(x.train), size = nrow(x.train), replace = TRUE)
   x.train.boot <- x.train[k,]
   y.train.boot <- y.train[k]
   data.lm <- lm(y.train.boot ~ as.matrix(x.train.boot))
   yhat <- as.matrix(cbind(1, x.val)) %*% as.matrix(data.lm$coefficients)
   yhats.boot[i] <- mean(yhat)
 }
 yhat.boot.10 <- sort(yhats.boot, decreasing = F)[100]
 yhat.boot.10

t <- rq(y.train~as.matrix(x.train), tau = 0.1)
t$coefficients
yhat.rq <- as.matrix(cbind(1, x.val)) %*% t$coefficients

plot(x=df.val$Date, y=df.val$Today, type = "l", xlab="Date", ylab="Today")
k = rep(yhat.boot.10, nrow(df.val))
lines(x=df.val$Date, y = k, col = "red")
lines(x=df.val$Date, y = yhat.rq, col = "blue")
```

train data에서 복원추출을 허용하여 train data의 개수만큼 뽑아낸다. 새롭게 뽑힌 데이터에서 lm 함수를 이용하여 회귀계수를 추정한 후, 이 회귀계수를 이용하여 예측값들을 구해낸다. 이 값들을 평균내어 yhats.boot에 저장한다. 이러한 과정을 1000회 반복하여 1000개의 예측값들을 얻어낸다. 그 후 이 값을 오름차순으로 sorting하여 100번째 값을 구하면 예측값들의 0.1 분위수를 추정한 것이 된다. 

그림에서 파란색 선은 rq함수를 이용했을 때 0.1 분위수를 추정한 것이다.
빨간색 선은 부트스트랩을 이용하여 구한 0.1분위수 추정량이다.

## (f)
** 그림과 코드에 대한 부분을 여기에 함께 작성하여 수기로 작성한 부분은 없습니다. **

위 그림을 볼 때, 우리가 예측한 것은 0.1 분위수 추정값이기 때문에 빨간색 선은 파란색 선을 대표해야 한다. 하지만 위 그림을 볼 때, 빨간색 선은 거의 파란색 윗부분에 존재하므로 대표한다고 보기 어렵다. 

부트스트랩을 평균을 하여 저장하면 데이터들의 분포를 알 수 있고, 이에 대한 오름차순 정렬에서 100번째 값을 뽑으면 0.1분위수에 해당하는 값을 가질 거라고 생각한게 위 (e) 문항이었다. 하지만 그림과 같이 예측하기에 부적절하다는 것을 볼 수 있다.


```{r}
summary(yhat.rq)
```

실제로 rq함수에 의한 값들은 위와 같다. 하지만 우리가 추정한 값은 아래와 같다.

```{r}
 yhat.boot.10
```

즉, 거의 max의 값에 해당하는 것을 볼 수 있다. 

또한 주어진 데이터는 시계열 데이터였다. 하지만, 이 데이터를 부트스트랩을 이용하면서 각각의 날짜에 따른 특성들이 무시되는 효과를 가져온다. (복원 추출을 하기 때문에 날짜 순서가 뒤죽박죽이 되게 된다.) 물론 시계열 데이터임을 잊고 위와 같이 코딩을 하여도 상당히 수긍하기 어려운 결과가 나오는 것을 볼 수 있다. 따라서 상사가 제안한 추정량이 F-1(0.1)의 값을 추정하기 어려울 것이다. 
