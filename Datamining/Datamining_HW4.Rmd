---
navbar:
  title: "datamining_HW4"
author : "01조"
date : "2020.10.14"
output:
  html_document:
    theme: cosmo
    highlight: textmate
    
---
>
## 조원 : 1711840 통계학과 한소현 **(최종 검토자 : 한소현)**
## 조원 : 1713924 통계학과 김소희
## 조원 : 1810618 통계학과 이은경

## library load
```{r library}
# library load
library(ISLR)
library(tree)
library(glmnet)

# 필요한지 확인
library(class)
library(pROC)
library(boot)
```



## 8.9 (풀이 : ddd / 기여 : ddd)

### (a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.
```{r}
data(OJ)
set.seed(1)
ind.train = sample(1:nrow(OJ), 800)
df.train = OJ[ind.train,]
df.test = OJ[-ind.train,]
```


### (b) Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?
```{r}
fit.tree = tree(Purchase~., data = df.train)
summary(fit.tree)
```
training set으로 tree를 적합한 결과, 실제로 사용된 variable은 LoyalCH, PriceDiff, SpecialCH, ListPriceDiff, PctDiscMM의 다섯 개이다. 9개의 terminal node인 tree가 생성될 것이다. training error rate은 127/800 = 0.1588이다.


### (c) Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.
```{r}
fit.tree
```
6번째 terminal node를 예로 들어보면, 먼저 (root 800 먼 뜻임?) LoyalCH가 0.5036보다 큰 브랜치로 435개가 존재할 것이다. 여기서 다시 LoyalCH가 0.764572보다 작은 node가 174개 선택될 것이고 이 노드의 deviance는 201.00일 것이다. 전반적으로 CH로 예측할 것이다.


### (d) Create a plot of the tree, and interpret the results.
```{r}
plot(fit.tree)
text(fit.tree, pretty = 0)
```


### (e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted tets labels. What is the test error rate?
```{r}
pred.tree = predict(fit.tree, df.test, type = "class")
table(pred.tree, df.test$Purchase)
```
오분류율은 (8+38)/(160+38+8+64) = 0.1703704로 약 17%이다.


### (f) 5-fold CV 사용
```{r}
K = 5
store.mse = numeric(K) #오분류율 저장
store.dev = numeric(K)
tree.size = numeric(K)

cv5.tree <- function() {
  for (k in 1:K) {
    set.seed(k*1004)
    n = nrow(df.train) #전체를 train으로 하는 게 맞나..
    ind.shf = sample(1:n, size = n)
    
    ind.val = ind.shf[(floor(n/K)*(k-1)+1):(floor(n/K)*k)]
    ind.tr = setdiff(1:n, ind.val)
    df.tr = df.train[ind.tr,]
    df.val = df.train[ind.val,]
    
    #cv.ind = sample(1:nrow(OJ), 800)
    #cv.train = OJ[cv.ind,]
    #cv.test = OJ[-cv.ind,]
    
    cv.fit.tree = tree(Purchase~., data = df.tr)
    cv.pred.tree = predict(cv.fit.tree, df.val, type = "class")
    t <- table(cv.pred.tree, df.val$Purchase)
    store.mse[k] = (t[2] + t[3]) / nrow(df.val)
    tree.size[k] = summary(cv.fit.tree)$size
    store.dev[k] = summary(cv.fit.tree)$dev
  }
  store = cbind(tree.size, store.mse, store.dev)
  return(store)
  #return(mean(store.mse))
}
store = cv5.tree()
#tree.size[which.min(store.mse)]
```
이것을 보면, 오분류율이 가장 작을 때의 tree size는 9임을 알 수 있다.


### (g) Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.
```{r}
error.stack = numeric(nrow(df.train))
#for (i in 1:nrow(df.train)){
#  error.stack[i] = cv.tree(i)
#}

#plot(tree.size, store.mse)
store = store[order(store[,1]),]

plot(store[,1], store[,2], type = "b")
plot(store[,1], store[,3], type = "b")
```


### (h) Which tree size corresponding to the optimal tree size obtained using cross-validated classification error rate?
```{r}
#prune.tree = prune.tree(fit.tree)
#size.seq = prune.tree$size
#dev.seq = prune.tree$dev
#plot(size.seq, dev.seq, type = "b")
i = 1

```


### (i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.
```{r}
K = 5
cv.error.stack = matrix(NA, ncol=5, nrow = 10)
# subtree들의 성능평가 # 5번 하는데, 각 시행시 size마다 계산.
for (k in 1:K) {
  set.seed(k*1004)
  n = nrow(df.train)
  ind.shf = sample(1:n, size = n)
    
  ind.val = ind.shf[(floor(n/K)*(k-1)+1):(floor(n/K)*k)]
  ind.tr = setdiff(1:n, ind.val)
  df.tr = df.train[ind.tr,]
  df.val = df.train[ind.val,]
  
  cv.fit.tree = tree(Purchase~., data = df.tr)
  cv.size.seq = prune.tree(cv.fit.tree)$size
  #cv.size.seq = cv.fit.tree$size
  print(cv.size.seq)
  for (i in 1:(length(cv.size.seq)-1)){
    mysize = cv.size.seq[i]
    tree.c.temp = prune.tree(cv.fit.tree, best = mysize)
    yhat = predict(tree.c.temp, newdata = df.val, type = "class")
    mis = (table(yhat, df.val$Purchase)[1,2] + table(yhat, df.val$Purchase)[2,1])/nrow(df.val)
    cv.error.stack[cv.size.seq[i],k] = mis
  }
}
cv.error.stack
error = rowSums(cv.error.stack)
best.size = which.min(error)

prune.tree = prune.tree(fit.tree, best = best.size)
```


### (j) Compare the training error rates between the pruned and unpruned trees. Which is higher?
```{r}
#summary(fit.tree)
#summary(prune.tree)
yhat = predict(prune.tree, df.train, type="class")
t <- table(yhat, df.train$Purchase)
miss = (t[2]+t[3])/nrow(df.train)
miss
```


### (k) Compare the test error rates between the pruned and unpruned trees. Which is higher?
```{r}
yhat.test = predict(prune.tree, df.test, type = "class")
t <- table(yhat.test, df.test$Purchase)
miss.test = (t[2]+t[3])/nrow(df.test)
miss.test
```



## 추가문제 (풀이 : ddd / 기여 : ddd)
### (a) 데이터셋 생성, 표본크기 
```{r}
expit = function(t) return(exp(t) / (1 + exp(t)))
p = 20
n = 1000
Xmat = matrix(1, nrow = n, ncol = 1)
beta = c(1,-2,1,1,0,-3,0,1,2,0,2,1,-1,-5,4,-6,2,0,-1,0,1)

set.seed(1)
for (i in 1:21) {
 beta[i] = round(rnorm(1), 2)
 if (beta[i] < -1.5 | beta[i] > 1.5) {
   beta[i] = 0
 }
}

for (j in 1:p) {
  Xmat = cbind(Xmat, rnorm(n))
}
y <- numeric(n)
for (i in 1:nrow(Xmat)) {
  x_theta = 0
  for (j in 1:length(beta)) {
    x_theta = x_theta + beta[j] * Xmat[i,j]
  }
  #print(x_theta)
  y[i] = rbinom(n=1, size=1, prob=expit(x_theta))
}
```


### (b) 훈련 세트와 테스트 세트로 분할 (100:900)
```{r}
total_data = cbind(Xmat, y)
ind.train = sample(1:1000, 900)
trainset = total_data[ind.train,]
testset = total_data[-ind.train,]
x.train = trainset[,2:(p+1)]
y.train = trainset[,'y']
x.test = testset[,2:(p+1)]
y.test = testset[,'y']
```


### (c) 라쏘 이진 로지스틱 회귀분석
```{r}
grid = 2^seq(50,-49,-1)
obj.lasso = glmnet(x= x.train, y = y.train, familly = "binomial", alpha = 1, lambda = grid)
coef(obj.lasso)[,15]
```


### (d) 시각화 : 람다 vs. 적합된 모형계수의 nonzero 성분 개수
```{r}
count = numeric(length(grid)) # nonzero counting
for (i in 1:length(grid)) {
  count[i] = sum(coef(obj.lasso)[,i] == 0)
}

plot(log2(grid), count)
```


### (e) 시각화 : 람다 vs. 적합된 모형의 훈련세트 오분류율
```{r}
mse.lasso <- numeric()
for (i in 1:length(grid)) {
  yhat.lasso = Xmat %*% coef(obj.lasso)[,i]
  mse = sqrt(sum((y.train-yhat.lasso)^2))
  mse.lasso[i] = mse
}

plot(log2(grid), mse.lasso)
```


### (f) 시각화 : 람다 vs. 적합된 모형의 테스트세트 오분류율
```{r}
mse.lasso.test <- numeric()

for (i in 1:length(grid)) {
  yhat.lasso.test = cbind(1,x.test) %*% coef(obj.lasso)[,i]
  mse = sqrt(sum((y.test-yhat.lasso.test)^2))
  mse.lasso.test[i] = mse
}

plot(log2(grid), mse.lasso.test)
```

### (g) 람다가 몇일 대 테스트세트 오분류율이 최소화?
그에 대응하는 적합계수의 자유도는 얼마?
```{r}
#앞에꺼가 trivial해서 못해 ㅜㅜㅜㅜㅜㅜ
```


### (h) 시각화 : 람다 vs. 적합된 모형계수의 l2추정오차
```{r}

```

