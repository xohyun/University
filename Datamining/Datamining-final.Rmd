---
title: "Datamining_final"
output: 
  html_document : 
    toc : TRUE
    theme: flatly
    highlight : textmate
    toc_depth : 3
    toc_float: 
      collapsed : false
---
```{r}
library(tree)
library(randomForest)
library(gbm)
```

# 1.
## (a) Write function bagging.class
```{r, warning = F}
bagging.class <- function(X, y, newdata, B, d) {
  size <- nrow(X)
  df <- cbind(X,y)
  colnames(df) <- data.name
  
  # predict stack
  yhat.stack <- data.frame(matrix(ncol = B, nrow = nrow(newdata)))
  
  for (i in 1:B) {
    boot.ind <- sample(1:size, floor(size*0.6), replace = T)
    train <- df[boot.ind,]
    
    train.tree <- tree(y~., data = train)
    prune.train.tree <- prune.tree(train.tree, best = d)
    
    yhat.stack[,i] <- predict(prune.train.tree, newdata = newdata, type = "class")
  }

  result.y <- numeric(m)
  for (i in 1:m) {
    t <- as.factor(yhat.stack[i,])
    levels(t) <- c(0,1)
    
    result.y[i] <- ifelse(table(t)[1] > table(t)[2], 0, 1)
  }
  return (result.y)
}
```

## (b) Use function
```{r}
B = 1000; d = 5; p = 4; m = 100; n = 1000
df.x <- data.frame(matrix(NA, ncol = p, nrow = n)) # train의 X
test.x <- data.frame(matrix(NA, ncol = p, nrow = m)) # test의 X

for (i in 1:p) {
  set.seed(i*10)
  df.x[,i] <- rnorm(n)*sample(c(-2,-1,1,2),1)
  set.seed(i*100)
  test.x[,i] <- rnorm(m)*sample(c(-2,-1,1,2),1)
}
y <- as.factor(ifelse(apply(df.x, 1, mean) > (-0.01), 1, 0)) # train의 y
df <- cbind(df.x, y = y)


# colname 짓기
data.name <- NULL
  for (i in 1:ncol(df.x)) {
    data.name <- c(data.name, paste0("x", i))
  }
colnames(test.x) <- data.name
data.name <- c(data.name, "y")
colnames(df) <- data.name

# 함수 호출
function.y <- bagging.class(df.x, y, test.x, B, d)

# 외부 라이브러리 함수 기반
obj.bag = randomForest(y ~ ., data = df, mtry = p, importance = TRUE, ntree = B, maxnodes = d)
yhat.bag = predict(object = obj.bag, newdata=test.x, type = "response")

# 얼마나 동일한가?
sum(function.y == yhat.bag) / m
```
외부라이브러리랑 91% 정도 비슷한 것을 알 수 있다. 


# 2.
## (a) Write function gbm.class

```{r}
gbm.class <- function(X, y, newdata, B, eps, d) {
  # B :  나무개수 / d : 나무의 크기(끝마디의 수) / eps : 기본 학습기의 축소계수
  
  naming <- NULL # 이름을 붙이는 벡터
  for (i in 1:ncol(X)) {
    naming <- c(naming, paste0("x", i))
  }
  naming <- c(naming, "r")
  
  count1 <- table(y)[2] # 1인 개수 세기
  G <- log(count1 / (length(y) - count1))
  
  #중간 예측값 저장하는 데이터 프레임
  store_stack <- data.frame(matrix(NA, ncol = nrow(newdata), nrow = B)) 
  
  for (i in 1:B) {
    r <- (-y) + (exp(G) / (1 + exp(G)))
    train <- cbind(X, r)
    colnames(train) <- naming
    gb <- tree(r~., data = train)
    prune.gb <- prune.tree(gb, best = d)
      
    gb.predict <- predict(prune.gb, newdata = train, type = "vector")
    G <- G + eps*gb.predict
  
    prob <- exp(G) / (1 + exp(G))
    yhat.pred <- ifelse(prob>=0.5, 1, 0)
    store_stack[i,] <- prob
  }
    
  #prob <- exp(G) / (1 + exp(G))
  #yhat.pred <- ifelse(prob > 0.5, 1 , 0)
  #적합된 tree를 이용해야 하는데, 위에서 적합된 트리는 B-1번이므로 한번더 적합.
  r <- -y + (exp(G) / (1 + exp(G)))
  train <- cbind(X, r)
  colnames(train) <- naming
  gb <- tree(r~., data = train)
  prune.gb <- prune.tree(gb, best = d)
  
  final.predict <- predict(prune.gb, newdata = newdata, type = "vector")
  final.prob <- exp(final.predict) / (1 + exp(final.predict))
  yhat.pred <- ifelse(final.prob >= 0.5, 1 , 0)
  return(yhat.pred)
}
```


## (b) Use function

데이터 생성을 위 1번과 다르게 생성하였다.

(Y|X follows the Bernoulli distribution 
with the success probability as expit(beta0 + beta %*% X)

```{r}
B = 10000; d = 4; p = 15; m = 100; n = 1000; eps = 0.8
##############################

df.x <- data.frame(matrix(NA, ncol = p, nrow = n)) # train의 X
test.x <- data.frame(matrix(NA, ncol = p, nrow = m)) # test의 X
expit = function(t) return(exp(t) / (1 + exp(t)))
theta.true = c(-2, -1, 1, 2)
for (i in 1:p) {
  set.seed(i*528)
  df.x[,i] <- rnorm(n)*5
  #x_theta <- x_theta + sample(theta.true, 1) * df.x[,i]
  test.x[,i] <- rnorm(m)
}

theta.true = runif(p+1, min = 0, max = 3)
x_theta = (as.matrix(cbind(1,df.x)) %*% theta.true)

y = rbinom(n=n, size=1, prob=expit(x_theta)) 

# for (i in 1:p) {
#   set.seed(i*100)
#   df.x[,i] <- rnorm(n)*sample(c(-2,-1,1,2),1)
#   set.seed(i*10)
#   test.x[,i] <- rnorm(m)*sample(c(-2,-1,1,2),1)
# }
# y <- ifelse(apply(df.x, 1, mean) > (-0.01), 1, 0) # train의 y
df <- cbind(df.x, y = y)

# colname 짓기
data.name <- NULL
  for (i in 1:ncol(df.x)) {
    data.name <- c(data.name, paste0("x", i))
  }
colnames(test.x) <- data.name
data.name <- c(data.name, "y")
colnames(df) <- data.name

# 함수 호출
result.y <- gbm.class(df.x, y, test.x, B, eps, d)

# epss로 eps를 모아놓은 벡터를 만들어서 어떤 eps에서 최대가 되는지 확인해보았다.
# epss <- seq(0.005, by = 0.005)
# result.stack <- as.numeric(length(epss))
# 
# for (i in 1:length(epss)) {
#   result.y <- gbm.class(df.x, y, test.x, B, epss[i], d)
#   
#   df$y <- as.character(df$y)
#   obj.gbm = gbm(y ~ ., data=df, distribution = "bernoulli", 
#     	              n.trees = B, shrinkage = epss[i], interaction.depth = 4)
#   yhat.gbm = predict(obj.gbm, newdata = test.x, type="response", n.tree = B) 
#   y.gbm <- ifelse(yhat.gbm>=0.5, 1, 0)
#   result.stack[i] <- sum(y.gbm == result.y) / m
# }

##############################
df$y <- as.character(df$y)
obj.gbm = gbm(y ~ ., data=df, distribution = "bernoulli", 
    	              n.trees = B, shrinkage = eps, interaction.depth = 3)
yhat.gbm = predict(obj.gbm, newdata = test.x, type="response", n.tree = B) 
y.gbm <- ifelse(yhat.gbm>=0.5, 1, 0)
sum(y.gbm == result.y) / m
```

데이터, B, d, p, eps의 값이 달라질 때마다, 매우 큰 차이의 결과를 보인다. 현재 보이는 95%는 가장 비슷한 수치를 보이는 모수로 setting을 해놓았다. 

추가로 데이터 생성을 1번과 동일하게 했을 때의 결과값이다. 같은 B, d, p, eps의 값인데도 불구하고 거의 비슷하지 않음을 볼 수 있다. 

```{r}
B = 10000; d = 4; p = 15; m = 100; n = 1000; eps = 0.8
##############################
df.x <- data.frame(matrix(NA, ncol = p, nrow = n)) # train의 X
test.x <- data.frame(matrix(NA, ncol = p, nrow = m)) # test의 X

for (i in 1:p) {
  set.seed(i*100)
  df.x[,i] <- rnorm(n)*sample(c(-2,-1,1,2),1)
  set.seed(i*10)
  test.x[,i] <- rnorm(m)*sample(c(-2,-1,1,2),1)
}
y <- ifelse(apply(df.x, 1, mean) > (-0.01), 1, 0) # train의 y
df <- cbind(df.x, y = y)

# colname 짓기
data.name <- NULL
  for (i in 1:ncol(df.x)) {
    data.name <- c(data.name, paste0("x", i))
  }
colnames(test.x) <- data.name
data.name <- c(data.name, "y")
colnames(df) <- data.name

# 함수 호출
result.y <- gbm.class(df.x, y, test.x, B, eps, d)

# epss로 eps를 모아놓은 벡터를 만들어서 어떤 eps에서 최대가 되는지 확인해보았다.
# epss <- seq(0.05, by = 0.05)
# result.stack <- as.numeric(length(epss))
# 
# for (i in 1:length(epss)) {
#   result.y <- gbm.class(df.x, y, test.x, B, epss[i], d)
#   
#   df$y <- as.character(df$y)
#   obj.gbm = gbm(y ~ ., data=df, distribution = "bernoulli", 
#     	              n.trees = B, shrinkage = epss[i], interaction.depth = 3)
#   yhat.gbm = predict(obj.gbm, newdata = test.x, type="response", n.tree = B) 
#   y.gbm <- ifelse(yhat.gbm>=0.5, 1, 0)
#   result.stack[i] <- sum(y.gbm == result.y) / m
# }

##############################
df$y <- as.character(df$y)
obj.gbm = gbm(y ~ ., data=df, distribution = "bernoulli", 
    	              n.trees = B, shrinkage = eps, interaction.depth = 3)
yhat.gbm = predict(obj.gbm, newdata = test.x, type="response", n.tree = B) 
y.gbm <- ifelse(yhat.gbm>=0.5, 1, 0)
sum(y.gbm == result.y) / m
```